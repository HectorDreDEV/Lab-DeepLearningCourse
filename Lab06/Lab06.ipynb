{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 06: Recurrent Neural Network (RNN)\n",
    "\n",
    "Trong bài thực hành này:\n",
    "- Cài đặt 1 mạng RNN cơ bản LSTM\n",
    "- Sử dụng Word Embedding GLOVE của Stanford\n",
    "- Chạy trên data spam detection\n",
    "\n",
    "Reference:\n",
    "- Glove: https://github.com/stanfordnlp/GloVe\n",
    "- LSTM: Long Short-Term Memory layer - Hochreiter 1997.\n",
    "\n",
    "Đọc thêm:\n",
    "- LSTM: https://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tiền xử lý dữ liệu\n",
    "\n",
    "Chúng ta cần tách câu thành từng từ trước."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "df = pd.read_csv(\"spam_detection.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df[\"Text\"].to_list()\n",
    "texts = [text.lower() for text in texts            # chuyển các đoạn text thành chữ thường (word embedding chỉ cho chữ thường)\n",
    "tokenized_texts = [nltk.tokenize.word_tokenize(text) for text in texts]    # tách câu thành một list các từ\n",
    "\n",
    "print(tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load embedding từ file\n",
    "\n",
    "Pretrained Embeddings từ Glove-Stanford đã được rút gọn cho bài tập này và lưu thành file glove_embedding.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## không cần hiểu đống này lắm đâu\n",
    "import io\n",
    "import numpy as np\n",
    "def load_word_embeddings(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    vocab, matrix = [], []\n",
    "    i=0\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        vocab.append(tokens[0])\n",
    "        matrix.append(list(map(float, tokens[1:])))\n",
    "    return vocab, np.asarray(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, matrix = load_word_embeddings(\"glove_embedding.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sau khi đọc xong thì\n",
    "- vocab: một danh sách các từ vực có trong embedding\n",
    "- matrix: một ma trận, mỗi dòng là một embedding cho từ tương ứng trong vocab (xếp đúng thứ tự)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Số hóa data\n",
    "\n",
    "Để số hóa 1 từ (word) trong ngôn ngữ tự nhiên, người ta sẽ biểu diễn từ đó thành một vector (gọi là embedding). 2 bước trước ta đã tách các câu trong data thành từ riêng biệt, và load một bộ embedding có sẵn. Bây giờ ta chuyển từng từ trong data thành một mã số biểu thị vị trí của từ đó trong ma trận embedding.\n",
    "\n",
    "Tuy nhiên, ta cần có vài mã số đặc biệt để giải quyết các vấn đề như: \n",
    "- từ không có trong embedding\n",
    "- Độ dài các câu không giống nhau. Cơ bản, các thư viện deep learning tính toán nhanh dựa trên các kĩ thuật tính toán ma trận (tensor), nên để tính các câu có độ dài ngắn khác nhau, các câu ngắn cần được nối thêm bởi các mã đặc biệt để có cùng kích thước.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gán các mã\n",
    "__PADDED_INDEX__ = 0    # mã dùng cho các vị trí chỉ có tính nối dài cho cùng kích thước\n",
    "__UNKNOWN_WORD__ = 1    # mã cho những từ không có trong embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo một dictionary, có nhiệm vụ là một ánh xạ từ ảnh sang mã số, mã số được bắt đầu từ 2 vì số 0 và 1 được dành cho trường hợp đặc biệt\n",
    "word_to_index = {word: index+2 for index, word in enumerate(vocab)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do do mã số được bắt đầu từ 2, nên cần thêm 2 vector vào đàu ma trận\n",
    "embedding_matrix = np.pad(matrix, [[2,0],[0,0]], mode='constant', constant_values =0.0)\n",
    "print(embedding_matrix)\n",
    "\n",
    "# Khi đó, __PADDED_INDEX__ dùng dòng đầu tiên của  embedding_matrix\n",
    "# __UNKNOWN_WORD__ dùng dòng thứ hai của embedding_matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bây giờ ta sẽ chuyển data spam dection thành các mã số\n",
    "import tensorflow as tf\n",
    "\n",
    "X = []\n",
    "for text in tokenized_texts:\n",
    "    cur_text_indices = []\n",
    "    for word in text:\n",
    "        if word in word_to_index:\n",
    "            cur_text_indices.append(word_to_index[word])    ## map từ word sang index\n",
    "        else:\n",
    "            cur_text_indices.append(__UNKNOWN_WORD__)       ## gán unknown cho từ không có trong bộ glove\n",
    "    X.append(cur_text_indices)\n",
    "\n",
    "## pad data cho có cùng chiều dài\n",
    "X = tf.keras.preprocessing.sequence.pad_sequences(sequences=X,       # sequences: list các câu có độ dài không bằng nhau\n",
    "                                                  padding='post')    # vị trí pad là 'pre' (trước) hoặc 'post' (sau)\n",
    "\n",
    "y = df['y'].values   ## Label của bài toán, 0 là không phải spam, 1 là spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chia data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size= 0.2, random_state =0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN trong tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Bidirectional, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "inputs = Input(shape=(None,))                   ## None biểu thị kích thước không xác định của câu\n",
    "\n",
    "embed = Embedding(input_dim=embedding_matrix.shape[0],   ## Khai báo kích thước của vocab\n",
    "                 output_dim=embedding_matrix.shape[1],   ## Khai báo kích thước của embedding\n",
    "                  embeddings_initializer = tf.keras.initializers.Constant(value=embedding_matrix),  ## Khởi tạo cho embedding bằng ma trận có sẵn\n",
    "                  trainable=False,                       ## Không cần thiết train embedding\n",
    "                 mask_zero=True)(inputs)                 ## zero_mask: những vị trí có giá trị 0 không được tính toán, vì đó là giá trị thêm vào cho đủ độ dài mà thôi\n",
    "                                                         ##  (__PADDED_INDEX__ gán bằng 0)\n",
    "\n",
    "lstm = LSTM(units=100,                          ## units: kích thước của hidden_state trong LSTM\n",
    "            return_sequences=False)(embed)      ## return_sequences: LSTM trả về toàn bộ  hay là trả về hidden_state cuối cùng\n",
    "\n",
    "dense = Dense(units=2, activation='softmax')(lstm)\n",
    "model = Model(inputs=inputs,\n",
    "              outputs=dense)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint Callback\n",
    "mc = tf.keras.callbacks.ModelCheckpoint(filepath=\"lstm_spam.h5\", \n",
    "                                     monitor='val_loss',\n",
    "                                     mode='min', \n",
    "                                     verbose=0, \n",
    "                                     save_best_only=True)\n",
    "# Train\n",
    "model.fit(X_train, y_train, \n",
    "          validation_data=(X_valid, y_valid),\n",
    "         epochs=10,\n",
    "         callbacks=[mc])\n",
    "\n",
    "model.load_weights(\"lstm_spam.h5\")\n",
    "_, val_acc = model.evaluate(X_valid, y_valid)\n",
    "print(\"Accuracy on valid: \", val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bài tập\n",
    "- Nghiên cứu sử dụng tf.keras.layers.Bidirectional trong model. Lưu lý: cập nhật lên tensorflow 2.0 để Bidirectional chạy đúng mask_zero.\n",
    "- Hãy viết một hàm\n",
    "```python\n",
    "def model_predict(text):\n",
    "    \n",
    "    return True/False\n",
    "```\n",
    "để đoán xem một đoạn text đưa vào có phải là tin nhắn spam không (các biến khác dùng global)\n",
    "- Tự điều chỉnh và train model (chỉnh lại train, valid, tiền xử lý, dùng word_embedding,... nếu muốn) rồi đoán xem các câu sau có phải spam không:\n",
    "    - \"wanna ask something? just send me a mess\"\n",
    "    - \"Urgent! You have won our competition's prize!! Please call us now.\"\n",
    "    - \"Call me to get a free holiday now\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
