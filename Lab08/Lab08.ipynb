{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 08: Reinforcement Learning\n",
    "- Thiết lập một bài toán đơn giản, huấn luyện Q-table và Deep Q-Agent giải bài toán\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thiết lập môi trường"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bài toán (Slide 37 - Q-Learning của thầy Đăng): \n",
    "- Trong một cái sân có 5 ô, đánh số từ 0 đến 4, người chơi sẽ ở một trong 5 ô tại một thời điểm, trò chơi bắt đầu với người chơi ở ô 0.\n",
    "- Tại một lượt, người chơi chọn một trong hai hành động \"FORWARD\" hoặc \"BACKWORD\":\n",
    "    - \"FORWARD\": tiến về phía trước một ô, tức là đang ở ô i thì đến ô i + 1, nếu đang ở ô 5 thì vẫn ở lại ô;\n",
    "    - \"BACKWARD\": trở về vị trí ô 0;\n",
    "- Khi chọn xong hành đồng, người chơi cần tung một đồng xu với xác suất mặt ngửa là $\\gamma$, khi bị mặt ngửa, người chơi phải chọn hành động ngược lại.\n",
    "- Khi tiến vào ô 0, hoặc đang ở ô 0 và thực hiện BACKWARD, thì người chơi nhận được 2 điểm.\n",
    "- Khi tiến vào ô 4, hoặc đang ở ô 4 và thực hiện FORWARD thì người chơi được 10 điểm\n",
    "- Chơi trong tổng cộng $n=10000$ lượt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Environment():\n",
    "    \"\"\"Định nghĩa Môi trường giả lập của bài toán\"\"\"\n",
    "    def __init__(self, gamma=0.05):\n",
    "        self.gamma = gamma            ## xác suất phải thực hiện hành động ngược lại \n",
    "        self.rewards = [2,0,0,0,10]   ## points cho các ô\n",
    "        self.cur_pos = 0              ## vị trí hiện tại của người chơi\n",
    "        \n",
    "    def reset(self):\n",
    "        ## Reset môi trường về trạng thái ban đầu, tức là về lại vị trí 0\n",
    "        self.cur_pos = 0    \n",
    "        return self.cur_pos\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Thực hiện hành động action với môi trường, action là str có giá trị 1 trong 2 \"backward\" và \"forward\"\n",
    "        trả về quan sát (vị trí hiện tại), và điểm nhận được \n",
    "        \"\"\"\n",
    "        reward = 0\n",
    "        flip = random.random() <= self.gamma    ##thả đồng xu xem có ngửa không\n",
    "        if flip:      ## phải làm ngược lại\n",
    "            if action == \"backward\":\n",
    "                action = \"forward\"\n",
    "            elif action == \"forward\":\n",
    "                action = \"backward\"\n",
    "            else:\n",
    "                raise Exception(\"error value\")\n",
    "                \n",
    "        if action == \"backward\":   ## quay lại ô 1\n",
    "            self.cur_pos = 0\n",
    "        elif action == \"forward\":  ## tiến 1 ô \n",
    "            if self.cur_pos < 4:\n",
    "                self.cur_pos += 1\n",
    "        reward = self.rewards[self.cur_pos]\n",
    "        return self.cur_pos, reward\n",
    "\n",
    "    \n",
    "def play(agent, n_steps=10000):\n",
    "    \"\"\"truyền vào 1 agent để chơi trò chơi\n",
    "    agent phải có phương thwusc take_action(obs), nhận vào quan sát và trả về hành động tiếp theo\n",
    "    trả về tổng điểm agent đạt được sau n_steps lượt \n",
    "    \"\"\"\n",
    "    env = Environment()\n",
    "    obs = env.reset()\n",
    "    total_rewards = 0\n",
    "    for step in range(n_steps):\n",
    "        action = agent.take_action(obs)\n",
    "        obs, reward = env.step(action)\n",
    "        total_rewards += reward\n",
    "    return total_rewards\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Agent\n",
    "Thử chơi trò chơi với hành động được chọn ngẫu nhiên 50 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class RandomAgent():\n",
    "    \n",
    "    def take_action(self, obs):\n",
    "        if random.random() <= 0.5:\n",
    "            return \"backward\"\n",
    "        return \"forward\"\n",
    "    \n",
    "agent = RandomAgent()\n",
    "score = play(agent)\n",
    "print(\"Final Score: \", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Learning\n",
    "Trong phần này ta sẽ huấn luyện một agent sử dụng Q-table để chơi.\n",
    "\n",
    "Trước tiên thiết lập Q-agent trước"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_action = {   ## số hóa hành động\n",
    "    0: \"forward\",    \n",
    "    1: \"backward\",\n",
    "}\n",
    "\n",
    "action_to_index = {\n",
    "    'forward': 0,\n",
    "    'backward': 1,\n",
    "}\n",
    "\n",
    "class QAgent():\n",
    "    \"\"\"Lớp định nghĩa Agent dùng 1 Q-table cho trước để chơi\n",
    "    \"\"\"\n",
    "    def __init__(self, Qtable):\n",
    "        \"\"\"Nhận vào 1 Q-table,\n",
    "        Bài toán có 2 hành động nên và 5 quan sát (5 vị trí) \n",
    "        Q-table là một ma trận 2 x 5\n",
    "        \"\"\"\n",
    "        self.Qtable = Qtable\n",
    "    \n",
    "    def take_action(self, obs):\n",
    "        \"\"\"Nhận một quan sát và trả về hành động\n",
    "        \"\"\"\n",
    "        action = np.argmax(self.Qtable[:, obs]) ## Với 1 quan sát, xem hành động nào có Q-value lớn nhất thì chọn hành động đó\n",
    "        return index_to_action[action] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bây giờ ta sẽ học Q-table một cách ngây thơ :\n",
    "- Chọn hành động tốt nhất dựa vào những gì đã học\n",
    "- Nếu điểm bằng nhau thì chọn ngẫu nhiên\n",
    "- Q-table cập nhất với discount = 0 (không xem xét tương lai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q-table\n",
    "import numpy as np\n",
    "Qtable = np.zeros((2, 5))\n",
    "\n",
    "\n",
    "\n",
    "env = Environment()   ##tạo môi trường giả lập\n",
    "obs = env.reset()     ## lấy quan sát đầu tiên\n",
    "\n",
    "for episode in range(100000):    ## train trong 100000 vòng lặp \n",
    "    if Qtable[0,obs] == Qtable[1,obs]:      ##nếu 2 action cho quan sát cùng điểm thì chọn đại\n",
    "        action_index = random.randint(0,1)   \n",
    "    else:\n",
    "        action_index = np.argmax(Qtable[:, obs])  ## không thì chọn action có điểm tốt nhất mà chọn \n",
    "    new_obs, reward = env.step(index_to_action[action_index])          ## cho hành động vào giả lập và nhận lại quan sát mới và điểm thưởng\n",
    "    Qtable[action_index, obs] += reward   ## cập nhận lại Q-table cho hành động \n",
    "    obs = new_obs\n",
    "print(Qtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = QAgent(Qtable)\n",
    "score = play(agent)\n",
    "print(\"Final Score: \", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q learning\n",
    "Giá trị Q-value được cập nhật như sau:\n",
    "$$Q(s_t, a_t) = Q(s_t, a_t) + \\alpha [r_{t+1} + \\lambda \\max_{a}Q(s_{t+1}, a) - Q(s_t, a_t)]$$\n",
    "với $\\alpha$ là learning rate, $\\lambda$ là discount rate, $s_t$ là quan sát thời điểm $t$ và $r_{t+1}$ là phần thưởng sau khi thực hiện hành động $a_t$ với quan sát $s_t$.\n",
    "\n",
    "Ngoài ra agent sẽ ngẫu nhiên thực hiện exploration với xác suất nào đó"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q-table\n",
    "\n",
    "lrn_rate = 0.1     ## tham số cho Q-learning \n",
    "discount = 0.95\n",
    "\n",
    "import numpy as np\n",
    "Qtable = np.zeros((2, 5))\n",
    "env = Environment()\n",
    "obs = env.reset()\n",
    "max_episodes = 100000\n",
    "\n",
    "def do_exploration(cur_ep, max_eps):    ## trả về xem có thực hiện exploration không \n",
    "    return random.random() <= 0.5   ##xác suất 0.5\n",
    "\n",
    "## Q-learning\n",
    "for episode in range(max_episodes): ## episode là mỗi lần update Qtable\n",
    "    if Qtable[0,obs] == Qtable[1,obs]: \n",
    "        action_index = random.randint(0,1)\n",
    "    else:\n",
    "        action_index = np.argmax(Qtable[:, obs])\n",
    "        if do_exploration(episode, max_episodes):  ## nếu thực hiện exploration thì làm hành động ngược lại \n",
    "            action_index = 1 - action_index\n",
    "    \n",
    "    new_obs, reward = env.step(index_to_action[action_index])  ## cho vào chạy giả lập\n",
    "    \n",
    "    \n",
    "    Qtable[action_index, obs] += lrn_rate*(reward + discount*np.max(Qtable[:, new_obs]) - Qtable[action_index, obs])\n",
    "    ## cập nhật lại Q-table \n",
    "    \n",
    "    \n",
    "    obs = new_obs\n",
    "print(Qtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = QAgent(Qtable)\n",
    "score = play(agent)\n",
    "print(\"Final Score: \", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-learning\n",
    "\n",
    "Bây giờ ta sẽ thay Q-table bằng một mạng neuron:\n",
    "- Mạng neuron nhận vào một quan sát, và trả về Q-value của các hành động\n",
    "- Input của mạng là 1 vector one-hot thể hiện vị trí hiện tại\n",
    "- Output là một vector 2 chiều, thể hiện q-value cho 2 hành động\n",
    "\n",
    "Với mỗi quan sát $s$ và hành động $a$, ta cố huấn luyện mạng sao cho q-value tiến tới\n",
    "$$Q^*(s,a) = R_{s,a} + \\gamma \\max_{a'} Q(s',a')$$\n",
    "với $\\gamma$ là discount rate, $s'$ và $R_{s,a}$ là là quan sát và phần thưởng sau khi thực hiện hành động $a$ với quan sát $s$;\n",
    "\n",
    "Với $Q*(s,a)$ là mục tiêu của $Q(s,a)$ mỗi vòng lặp, làm loss sẽ là\n",
    "$$\\frac{1}{m}\\sum_{s,a} (Q(s,a) - Q^*(s,a))^2$$\n",
    "tuy nhiên do mạng nơ-ron $nn$ nhận $s$ trả về $a$ nên hàm loss:\n",
    "$$\\frac{1}{m}\\sum_{(s,a)} (nn_a(s) - nn_a^*(s))^2$$\n",
    "với chỉ số $a$ là lấy giá trị tương ứng với $a$ trong vector output, $nn_a^*$ là giá trị cố định không dùng để học trong hàm loss.\n",
    "Để code hàm loss này cần phải tự định nghĩa trong keras (xem code dưới)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "index_to_action = {\n",
    "    0: \"forward\",\n",
    "    1: \"backward\",\n",
    "}\n",
    "\n",
    "class DeepQAgent():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        ## build a neural network\n",
    "        inputs = Input(shape=(5,))\n",
    "        dense = Dense(units=10, activation='relu')(inputs)\n",
    "        outputs = Dense(units=2)(dense)\n",
    "        \n",
    "        self.model = Model(inputs, outputs)  ##model input là 5, output là 2\n",
    "        self.model.compile(optimizer='adam',\n",
    "                          loss=self._loss_function)   ## hàm loss tự định nghĩa\n",
    "     \n",
    "    def _loss_function(self, target, output):\n",
    "        \"\"\"output là output của NN, là ma trận (batch_size, 2)\n",
    "        output là 2 giá trị tuy nhiên chỉ có một giá trị trong hàm loss nên ta cần tách ra và bỏ giá trị ko cần khỏi loss\n",
    "        ta quy định nhãn cho 1 sample có 2 giá trị, target q-value và action tương ứng\"\"\"\n",
    "        actions = tf.cast(target[:, 0], tf.int32)    ## cột đầu quy định là action,\n",
    "        actions = tf.one_hot(actions, depth=2, dtype=tf.float32)  ## chuyển action thành onehot\n",
    "        target_values = target[:,1:]        ## lấy target value của action\n",
    "        target_values = target_values*actions   ## đổi sang ma trận, (batch_size, 2), những chỗ không có trong loss có giá trị không\n",
    "        return tf.reduce_mean(tf.square(target_values - output*actions))  \n",
    "        ## tính mean square error, mấy chỗ không có trong loss đều là hằng 0 nên không được tính loss\n",
    "    \n",
    "    def _get_action_values(self, obs):\n",
    "        \"\"\"nhận 1 quan sát obs dưới dạng số nguyên, trả về vector q-values cho các hành động\"\"\"\n",
    "        obs_onehot = np.zeros((1, 5))\n",
    "        obs_onehot[0, obs] = 1        ##chuyển quan sát thành one-hot\n",
    "        return self.model.predict(obs_onehot)\n",
    "    \n",
    "    def _take_action(self, obs):\n",
    "        \"\"\"nhận về quan sát obs, trả về action tốt nhất dạng index\"\"\"\n",
    "        values = self._get_action_values(obs)\n",
    "        return np.argmax(values, axis=-1)\n",
    "    \n",
    "    def take_action(self, obs):\n",
    "        \"\"\"nhận 1 quan sát, trả về hành động tốt nhất\"\"\"\n",
    "        action = self._take_action(obs)[0]\n",
    "        return index_to_action[action]\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "agent = DeepQAgent()\n",
    "print(agent.model.predict(np.eye(5)))  ## xem thửu q-table của agent hởi tạo ngẫu nhiên\n",
    "score = play(agent)\n",
    "print(\"Final Score: \", score)      ## chơi thử"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q-learning\n",
    "def do_exploration():\n",
    "    return random.random() <= 0.5\n",
    "\n",
    "batch_size = 32\n",
    "env = Environment(gamma=0.05)\n",
    "obs = env.reset()\n",
    "agent = DeepQAgent()\n",
    "freezed_agent = DeepQAgent()\n",
    "for episode in range(5000):    ## episode là mỗi lần update Qtable\n",
    "    \n",
    "    ##tạo sẵn numpy lưu obs và target q-value tương ứng\n",
    "    memory_observations = np.zeros((batch_size, 5))   #lưu dạng one-hot để input vào model luôn\n",
    "    target_Q_values = np.empty((batch_size, 2))  #label, cột đầu tiên lưu action, cột thứ 2 lưu target q-value\n",
    "    \n",
    "    if episode % 100 == 0:\n",
    "        ## xem coi agent chơi sao \n",
    "        print(agent.model.predict(np.eye(5)).T)\n",
    "        score = play(agent)\n",
    "        print(\"Score at {}: {}\".format(episode,score))\n",
    "        \n",
    "    if episode % 10 == 0:\n",
    "        freezed_agent.model.set_weights(agent.model.get_weights())\n",
    "        \n",
    "    for m in range(batch_size):   ## vòng lặp này để lần lượt chạy giả lập obs và reward\n",
    "\n",
    "        ## các bước chạy giả lập với obs\n",
    "        action = agent.take_action(obs)     ## lấy hành động\n",
    "        act_idx = action_to_index[action]   ## lấy index của hành độgn\n",
    "        if do_exploration():                ## nếu exploration thì làm hành động ngược\n",
    "            act_idx = 1-act_idx\n",
    "            action = index_to_action[act_idx]\n",
    "        new_obs, reward = env.step(action)          ## lấy reward của hành động\n",
    "        values = freezed_agent._get_action_values(new_obs)  \n",
    "        max_value = np.max(values)                  ## lấy max q-value của observation mới\n",
    "        \n",
    "        ##lưu obs, action và target q-value\n",
    "        memory_observations[m,obs] = 1 \n",
    "        target_Q_values[m, 0] = act_idx\n",
    "        target_Q_values[m, 1] = reward + 0.95*max_value\n",
    "        obs = new_obs\n",
    "        \n",
    "        \n",
    "    ## train với input là các observations và label là action và target q-value của nó \n",
    "    agent.model.train_on_batch(memory_observations, target_Q_values)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = play(agent)\n",
    "print(\"Final Score: \", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bài tập\n",
    "Hãy tự định nghĩa một trò chơi, thiết lập giả lập environment và huấn luyện một deep Q-learning để chơi trò đó\n",
    "- Trò chơi phức tạp xíu, nhưng đừng quá phức tạp lại fail mất, ít nhất là 20 observations và 2 actions nhé\n",
    "- Mô tả trò chơi kĩ xíu, viết hẳn environment và comment vào\n",
    "- Huấn luyện một deep Q-learning, lưu lại model để có gì anh chơi thử\n",
    "- Thời gian làm bài cho bài thực hành này là 2 tuần"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
